import 'dart:async';
import 'dart:developer';
import 'dart:io';
import 'dart:convert';
import 'dart:isolate';
import 'dart:typed_data';
import 'package:flutter/services.dart';
import 'package:image/image.dart' as img;
import 'package:path_provider/path_provider.dart';
import 'package:tflite_flutter/tflite_flutter.dart';

// --- Isolate（別部署）で実行されるコード ---

/// Isolateのエントリーポイント（入口）。メインスレッドからSendPortを受け取る。
void _aiIsolateEntry(Map<String, dynamic> message) async {
  // ★★★ 渡されたトークンとSendPortを取得 ★★★
  final mainSendPort = message['sendPort'] as SendPort;
  final token = message['token'] as RootIsolateToken;

  // ★★★ 渡されたトークンを使って初期化 ★★★
  BackgroundIsolateBinaryMessenger.ensureInitialized(token);
  // BackgroundIsolateBinaryMessenger.ensureInitialized(RootIsolateToken.instance!);

  final isolateReceivePort = ReceivePort();
  mainSendPort.send(isolateReceivePort.sendPort); // 自分の連絡先をメインスレッドに送る

  // モデルとラベルを一度だけロード
  // final modelPath = '${(await getApplicationSupportDirectory()).path}/model.tflite';
  // final modelPath = '${(await getApplicationSupportDirectory()).path}/wd-eva02-large-tagger-v3_float16.tflite';
  // final modelPath = '${(await getApplicationSupportDirectory()).path}/wd-v1-4-moat-tagger-v2_float32.tflite';
  final modelPath =
      '${(await getApplicationSupportDirectory()).path}/wd-vit-tagger-v3_float32.tflite';
  // final labelPath = '${(await getApplicationSupportDirectory()).path}/tags.csv';
  final labelPath =
      '${(await getApplicationSupportDirectory()).path}/selected_tags.csv';
  log('AI Isolate: Loading model from $modelPath and labels from $labelPath');

  Interpreter? interpreter;
  List<String>? labels;

  try {
    interpreter = await Interpreter.fromFile(File(modelPath));
    final labelString = await File(labelPath).readAsString();
    labels = labelString
        .split('\n')
        .where((line) => line.isNotEmpty)
        .map((line) {
          final parts = line.split(',');
          return parts.length > 1 ? parts[1].replaceAll('"', '').trim() : '';
        })
        .where((tag) => tag.isNotEmpty)
        .toList();
    log('AI Isolate: Model and labels loaded successfully.');
  } catch (e) {
    log('AI Isolate: Failed to load model or labels: $e');
  }

  log('AI Isolate: Ready to receive messages.');
  await for (final message in isolateReceivePort) {
    if (message is List && message.length == 2) {
      // 小包（List）としてメッセージを受け取る
      final String filePath = message[0]; // 1つ目の中身はファイルパス
      final SendPort replyPort = message[1]; // 2つ目の中身は返信用の伝票

      if (interpreter == null || labels == null) {
        replyPort.send(['AIモデル未ロード']);
        continue;
      }

      try {
        // final recognizedTags = _analyze(interpreter, labels, filePath);
        // replyPort.send(recognizedTags); // 受け取った伝票を使って結果を返信する
        final result = _analyze(
          interpreter,
          labels,
          filePath,
        ); // _analyzeはList<dynamic>を返す
        replyPort.send(result); // 解析結果をメインスレッドに送り返す
      } catch (e) {
        log('AI Isolate: Error during analysis: $e');
        replyPort.send(['AI解析エラー']);
      }
    } else if (message == 'close') {
      interpreter?.close();
      isolateReceivePort.close();
      break;
    }
  }
  log('AI Isolate: Closed.');
  // メインスレッドからのメッセージを待つ
  log('AI Isolate: Closed.');
}

/*
/// Isolate内で実行される純粋な解析ロジック
List<String> _analyze(Interpreter interpreter, List<String> labels, String filePath) {
  log("解析を開始：$filePath");
  const int inputSize = 448;
  
  // --- 前処理 ---
  final imageFile = File(filePath);
  final imageBytes = imageFile.readAsBytesSync();
  img.Image? image = img.decodeImage(imageBytes);
  if (image == null) throw Exception('Failed to decode');
  
  final resizedImage = img.copyResizeCropSquare(image, size: inputSize);
  final mean = [0.485, 0.456, 0.406];
  final std = [0.229, 0.224, 0.225];
  final inputTensor = List.generate(1, (_) => List.generate(inputSize, (_) => List.generate(3, (_) => List.generate(inputSize, (_) => 0.0))));
  for (int y = 0; y < inputSize; y++) {
    for (int x = 0; x < inputSize; x++) {
      final pixel = resizedImage.getPixel(x, y);
      inputTensor[0][y][0][x] = ((pixel.r / 255.0) - mean[0]) / std[0];
      inputTensor[0][y][1][x] = ((pixel.g / 255.0) - mean[1]) / std[1];
      inputTensor[0][y][2][x] = ((pixel.b / 255.0) - mean[2]) / std[2];
    }
  }

  // --- 推論 ---
  var output = List.filled(1 * labels.length, 0.0).reshape([1, labels.length]);
  interpreter.run(inputTensor, output);

  // --- 結果解析 ---
  const double confidenceThreshold = 0.35;
  List<String> recognizedTags = [];
  for (var i = 0; i < labels.length; i++) {
    if (output[0][i] > confidenceThreshold) {
      recognizedTags.add(labels[i].replaceAll('_', ' '));
    }
  }
  return recognizedTags.isEmpty ? ['タグが見つかりませんでした'] : recognizedTags;
}
*/
// in lib/services/ai_service.dart

/// Isolate内で実行される純粋な解析ロジック
List<String> _analyze(
  Interpreter interpreter,
  List<String> labels,
  String filePath,
) {
  log("解析を開始：$filePath");
  const int inputSize = 448;

  // --- 前処理 ---
  final imageFile = File(filePath);
  final imageBytes = imageFile.readAsBytesSync();
  img.Image? image = img.decodeImage(imageBytes);
  if (image == null) throw Exception('Failed to decode: $filePath');

  final resizedImage = img.copyResizeCropSquare(image, size: inputSize);

  // ★★★ [1, C, H, W] (Channels-First) 形式にデータを整形 ★★★
  // かつ、RGB -> BGR 順への変換も考慮（一般的なViTモデルの要件）
  final mean = [0.485, 0.456, 0.406]; // ImageNetの標準RGB平均
  final std = [0.229, 0.224, 0.225]; // ImageNetの標準RGB標準偏差

  // float32の一次元配列を準備
  final inputFloats = Float32List(1 * 3 * inputSize * inputSize); // 1, C, H, W
  int pixelIndex = 0; // resizedImageのピクセルインデックス

  for (int y = 0; y < inputSize; y++) {
    for (int x = 0; x < inputSize; x++) {
      final pixel = resizedImage.getPixel(x, y);

      // BGR順で格納。平均と標準偏差もそれに合わせて調整
      // inputFloats[pixelIndex + (0 * inputSize * inputSize)] = (((pixel.b / 255.0) - mean[2]) / std[2]); // Blue
      // inputFloats[pixelIndex + (1 * inputSize * inputSize)] = (((pixel.g / 255.0) - mean[1]) / std[1]); // Green
      // inputFloats[pixelIndex + (2 * inputSize * inputSize)] = (((pixel.r / 255.0) - mean[0]) / std[0]); // Red

      // ★★★ しかし、多くのモデルは内部でRGBとして学習されているため、
      // ★★★ 一旦RGBで試すのが安全。ダメなら上記のBGRコメントアウトを試す。
      inputFloats[pixelIndex + (0 * inputSize * inputSize)] =
          (((pixel.r / 255.0) - mean[0]) / std[0]); // Red
      inputFloats[pixelIndex + (1 * inputSize * inputSize)] =
          (((pixel.g / 255.0) - mean[1]) / std[1]); // Green
      inputFloats[pixelIndex + (2 * inputSize * inputSize)] =
          (((pixel.b / 255.0) - mean[2]) / std[2]); // Blue
    }
  }

  // TFLiteが期待する形状 [1, 3, 448, 448] に変形
  final inputTensor = inputFloats.reshape([1, 3, inputSize, inputSize]);

  // --- 推論 ---
  // outputのサイズはlabels.lengthと一致させる
  var output = List.filled(1 * labels.length, 0.0).reshape([1, labels.length]);
  interpreter.run(inputTensor, output);

  // --- 結果解析 ---
  const double confidenceThreshold = 0.35;
  List<String> recognizedTags = [];
  for (var i = 0; i < labels.length; i++) {
    if (output[0][i] > confidenceThreshold) {
      recognizedTags.add(labels[i].replaceAll('_', ' '));
    }
  }
  // ★★★ 前処理後の画像をBase64エンコードして送り返すための準備 ★★★
  // ここで一旦画像を8bit-RGBに変換し直してからエンコードする
  final processedImageForDisplay = img.copyResize(
    resizedImage,
    width: 128,
    height: 128,
  ); // 表示用に小さくする
  final pngBytes = img.encodePng(processedImageForDisplay);
  final base64Image = base64Encode(pngBytes);

  // 結果と一緒にBase64画像を返す
  return recognizedTags.isEmpty
      ? ['タグが見つかりませんでした', base64Image] // タグと画像をリストで返す
      : [...recognizedTags, base64Image]; // タグのリストに画像を追加して返す
}

// --- メインスレッドで実行されるコード ---

class AiService {
  static final AiService _instance = AiService._internal();
  factory AiService() => _instance;

  String? _latestAnalyzedImageBase64;
  String? get latestAnalyzedImageBase64 => _latestAnalyzedImageBase64;

  Isolate? _isolate;
  SendPort? _isolateSendPort;
  ReceivePort? _mainReceivePort;
  final Completer<void> _isolateReadyCompleter = Completer<void>();

  AiService._internal() {
    _initIsolate();
  }

  /// Isolateを初期化し、双方向通信を確立する
  Future<void> _initIsolate() async {
    _mainReceivePort = ReceivePort();
    final token = RootIsolateToken.instance;
    if (token == null) {
      log('Could not get RootIsolateToken');
      return;
    }
    // _isolate = await Isolate.spawn(_aiIsolateEntry, _mainReceivePort!.sendPort);
    _isolate = await Isolate.spawn(
      _aiIsolateEntry,
      // ★★★ SendPortと一緒にトークンも渡す ★★★
      {'sendPort': _mainReceivePort!.sendPort, 'token': token},
    );

    // Isolateから送られてくるメッセージをリッスン
    _mainReceivePort!.listen((message) {
      if (message is SendPort) {
        // Isolateから連絡先が送られてきた
        _isolateSendPort = message;
        // これでIsolateと通信できる準備が整った
        _isolateReadyCompleter.complete();
        log('AI Service: Isolate connection established.');
      }
    });
  }

  /// 画像解析をIsolateに依頼する
  Future<List<String>> analyzeImage(File imageFile) async {
    // Isolateの準備ができるまで待つ
    await _isolateReadyCompleter.future;

    if (_isolateSendPort == null) {
      log('AI Service: Isolate is not ready.');
      return ['AIサービス未準備'];
    }

    final completer = Completer<List<String>>();
    final tempReceivePort = ReceivePort();

    // Isolateからの返信を一度だけリッスン
    tempReceivePort.listen((message) {
      /*
      if (message is List) {
        // 結果はList<dynamic>で来るのでキャスト
        completer.complete(message.cast<String>());
      }
      */
      if (message is List && message.isNotEmpty) {
        // メッセージの最後の要素がBase64画像文字列であると仮定
        final List<dynamic> rawResult = message;
        String? base64Image;
        List<String> tags = [];

        if (rawResult.last is String && rawResult.last.length > 100) {
          // Base64の最低限の長さで判定
          base64Image = rawResult.removeLast() as String;
        }
        tags = rawResult.cast<String>();

        completer.complete(tags);
        // ★★★ ここでAIServiceに画像を保持させる ★★★
        _latestAnalyzedImageBase64 = base64Image;
      }
      tempReceivePort.close();
    });

    // Isolateに、画像のパスと、返信先の連絡先を送る
    _isolateSendPort!.send([imageFile.path, tempReceivePort.sendPort]);

    return completer.future;
  }

  /// アプリ終了時などにIsolateをクリーンアップする
  void dispose() {
    _isolateSendPort?.send('close');
    _mainReceivePort?.close();
    _isolate?.kill(priority: Isolate.immediate);
    _isolate = null;
    _latestAnalyzedImageBase64 = null;
    log('AI Service: Disposed.');
  }
}

/*
import 'dart:io';
import 'dart:developer';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:image/image.dart' as img;
import 'package:tflite_flutter/tflite_flutter.dart';

class AiService {
  Interpreter? _interpreter;
  List<String>? _labels;

  static const String _modelPath = 'assets/models/model.tflite';
  static const String _labelPath = 'assets/models/tags.csv';
  static const int _inputSize = 448;

  static final AiService _instance = AiService._internal();
  factory AiService() => _instance;
  AiService._internal();

  Future<void> loadModel() async {
    if (_interpreter != null) return;

    try {
      final options = InterpreterOptions();
      if (Platform.isAndroid) {
        options.addDelegate(GpuDelegateV2());
      } else if (Platform.isIOS) {
        options.addDelegate(GpuDelegate());
      }

      _interpreter = await Interpreter.fromAsset(_modelPath, options: options);
      log('AI Model loaded successfully with GPU delegate.');
    } catch (e) {
      log('GPU delegate failed, fallback to CPU: $e');
      try {
        _interpreter = await Interpreter.fromAsset(_modelPath);
        log('AI Model loaded successfully with CPU.');
      } catch (e2) {
        log('CPU load failed: $e2');
      }
    }

    if (_interpreter != null) {
      final labelString = await rootBundle.loadString(_labelPath);
      _labels = labelString
          .split('\n')
          .where((line) => line.isNotEmpty)
          .map((line) {
            final parts = line.split(',');
            if (parts.length > 1) {
              return parts[1].replaceAll('"', '').trim();
            }
            return '';
          })
          .where((tag) => tag.isNotEmpty)
          .toList();
      log('Labels loaded successfully. Total: ${_labels!.length}');
    }
  }

  void dispose() {
    _interpreter?.close();
    _interpreter = null;
    log('AI Model disposed.');
  }

  /// Isolate経由で画像を解析
  Future<List<String>> analyzeImage(File imageFile) async {
    if (_interpreter == null || _labels == null) {
      await loadModel();
      if (_interpreter == null || _labels == null) {
        return ['AIモデル未ロード'];
      }
    }

    try {
      final args = _AnalyzeArgs(
        imagePath: imageFile.path,
        inputSize: _inputSize,
        labels: _labels!,
        interpreterAddress: _interpreter!.address,
      );

      return await compute<_AnalyzeArgs, List<String>>(_analyzeIsolate, args);
    } catch (e) {
      log('Error during image analysis: $e');
      return ['AI解析エラー'];
    }
  }
}

/// Isolateに渡すデータ
class _AnalyzeArgs {
  final String imagePath;
  final int inputSize;
  final List<String> labels;
  final int interpreterAddress; // InterpreterはIsolate間で渡せないのでアドレス指定

  _AnalyzeArgs({
    required this.imagePath,
    required this.inputSize,
    required this.labels,
    required this.interpreterAddress,
  });
}

/// Isolateで実行される処理（トップレベル関数）
Future<List<String>> _analyzeIsolate(_AnalyzeArgs args) async {
  final imageFile = File(args.imagePath);
  final imageBytes = await imageFile.readAsBytes();
  img.Image? image = img.decodeImage(imageBytes);

  if (image == null) {
    return ['画像デコード失敗'];
  }

  final resizedImage = img.copyResizeCropSquare(image, size: args.inputSize);

  final mean = [0.485, 0.456, 0.406];
  final std = [0.229, 0.224, 0.225];

  final input = List.generate(
    1,
    (i) => List.generate(
      args.inputSize,
      (y) => List.generate(args.inputSize, (x) {
        final pixel = resizedImage.getPixel(x, y);
        final r = pixel.r / 255.0;
        final g = pixel.g / 255.0;
        final b = pixel.b / 255.0;
        return [
          (r - mean[0]) / std[0],
          (g - mean[1]) / std[1],
          (b - mean[2]) / std[2],
        ];
      }),
    ),
  );

  var output = List.filled(
    1 * args.labels.length,
    0.0,
  ).reshape([1, args.labels.length]);

  // Interpreter.fromAddressで同じインスタンスを再利用
  final interpreter = Interpreter.fromAddress(args.interpreterAddress);
  interpreter.run(input, output);

  const double confidenceThreshold = 0.35;
  List<String> recognizedTags = [];
  for (var i = 0; i < args.labels.length; i++) {
    if (output[0][i] > confidenceThreshold) {
      recognizedTags.add(args.labels[i].replaceAll('_', ' '));
    }
  }

  if (recognizedTags.isEmpty) return ['タグが見つかりませんでした'];
  return recognizedTags;
}

import 'dart:io';
import 'dart:developer';

import 'package:image/image.dart' as img;
import 'package:flutter/services.dart';
import 'package:flutter/foundation.dart';
import 'package:path_provider/path_provider.dart';
import 'package:tflite_flutter/tflite_flutter.dart';


// ★★★ Isolate（別部署）で実行される、自己完結した解析関数 ★★★
Future<List<String>> _analyzeImageIsolate(String filePath) async {
  // Isolateはメモリを共有しないため、モデルとラベルをここで再度読み込む必要がある
  // final directory = await getApplicationSupportDirectory();
  // final modelPath = '${directory.path}/model.tflite';
  // final labelPath = '${directory.path}/tags.csv';
  const String modelPath = 'assets/models/model.tflite';
  const String labelPath = 'assets/models/tags.csv';
  const int inputSize = 448;

  try {
    final interpreter = await Interpreter.fromAsset(modelPath);
    final labelString = await rootBundle.loadString(labelPath);
    final labels = labelString
        .split('\n')
        .where((line) => line.isNotEmpty)
        .map((line) {
          final parts = line.split(',');
          return parts.length > 1 ? parts[1].replaceAll('"', '').trim() : '';
        })
        .where((tag) => tag.isNotEmpty)
        .toList();

    // --- 画像の前処理 ---
    final imageFile = File(filePath);
    final imageBytes = await imageFile.readAsBytes();
    img.Image? image = img.decodeImage(imageBytes);
    if (image == null) throw Exception('Failed to decode');

    final resizedImage = img.copyResizeCropSquare(image, size: inputSize);
    final mean = [0.485, 0.456, 0.406];
    final std = [0.229, 0.224, 0.225];
    final inputTensor = List.generate(
      1,
      (_) => List.generate(
        inputSize,
        (_) => List.generate(3, (_) => List.generate(inputSize, (_) => 0.0)),
      ),
    );
    for (int y = 0; y < inputSize; y++) {
      for (int x = 0; x < inputSize; x++) {
        final pixel = resizedImage.getPixel(x, y);
        inputTensor[0][y][0][x] = ((pixel.r / 255.0) - mean[0]) / std[0];
        inputTensor[0][y][1][x] = ((pixel.g / 255.0) - mean[1]) / std[1];
        inputTensor[0][y][2][x] = ((pixel.b / 255.0) - mean[2]) / std[2];
      }
    }

    // --- 推論の実行 ---
    var output = List.filled(
      1 * labels.length,
      0.0,
    ).reshape([1, labels.length]);
    interpreter.run(inputTensor, output);
    interpreter.close();

    // --- 結果の解析 ---
    const double confidenceThreshold = 0.35;
    List<String> recognizedTags = [];
    for (var i = 0; i < labels.length; i++) {
      if (output[0][i] > confidenceThreshold) {
        recognizedTags.add(labels[i].replaceAll('_', ' '));
      }
    }
    return recognizedTags.isEmpty ? ['タグが見つかりませんでした'] : recognizedTags;
  } catch (e) {
    log('Error in _analyzeImageIsolate: $e');
    return ['AI解析エラー'];
  }
}

class AiService {
  static final AiService _instance = AiService._internal();
  factory AiService() => _instance;
  AiService._internal();

  // ★★★ AiServiceの役割は、compute経由で別部署の関数を呼び出すだけのシンプルなものになる ★★★
  Future<List<String>> analyzeImage(File imageFile) async {
    try {
      // compute関数が、_analyzeImageIsolateを別のIsolateで実行してくれる
      return await compute(_analyzeImageIsolate, imageFile.path);
    } catch (e) {
      log('Error calling compute: $e');
      return ['AI解析エラー'];
    }
  }
}
*/